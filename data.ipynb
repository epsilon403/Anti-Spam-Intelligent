{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "def2f8fe",
   "metadata": {},
   "source": [
    "# Anti-Spam Email Classification\n",
    "## 1. Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f18ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ExampleApp\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52a5bf9",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994a99af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "+---+----------+--------------------------------------------------+-----+----------+--------------------------------------------------+--------------------------------------------------+----------+\n",
      "|_c0|message_id|                                              text|label|label_text|                                           subject|                                           message|      date|\n",
      "+---+----------+--------------------------------------------------+-----+----------+--------------------------------------------------+--------------------------------------------------+----------+\n",
      "|  0|     33214|any software just for 15 $ - 99 $ understanding...|    1|      spam|                 any software just for 15 $ - 99 $|understanding oem software\\nlead me not into te...|2005-06-18|\n",
      "|  1|     11929|perspective on ferc regulatory action client co...|    0|       ham|perspective on ferc regulatory action client co...|19 th , 2 : 00 pm edt\\nperspective on ferc regu...|2001-06-19|\n",
      "|  2|     19784|wanted to try ci 4 lis but thought it was way t...|    1|      spam|wanted to try ci 4 lis but thought it was way t...|viagra at $ 1 . 12 per dose\\nready to boost you...|2004-09-11|\n",
      "|  3|      2209|enron / hpl actuals for december 11 , 2000 teco...|    0|       ham|        enron / hpl actuals for december 11 , 2000|teco tap 30 . 000 / enron ; 120 . 000 / hpl gas...|2000-12-12|\n",
      "|  4|     15880|looking for cheap high - quality software ? rot...|    1|      spam|looking for cheap high - quality software ? rot...|water past also , burn , course . gave country ...|2005-02-13|\n",
      "+---+----------+--------------------------------------------------+-----+----------+--------------------------------------------------+--------------------------------------------------+----------+\n",
      "only showing top 5 rows\n",
      "+---+----------+--------------------------------------------------+-----+----------+--------------------------------------------------+--------------------------------------------------+----------+\n",
      "|_c0|message_id|                                              text|label|label_text|                                           subject|                                           message|      date|\n",
      "+---+----------+--------------------------------------------------+-----+----------+--------------------------------------------------+--------------------------------------------------+----------+\n",
      "|  0|     33214|any software just for 15 $ - 99 $ understanding...|    1|      spam|                 any software just for 15 $ - 99 $|understanding oem software\\nlead me not into te...|2005-06-18|\n",
      "|  1|     11929|perspective on ferc regulatory action client co...|    0|       ham|perspective on ferc regulatory action client co...|19 th , 2 : 00 pm edt\\nperspective on ferc regu...|2001-06-19|\n",
      "|  2|     19784|wanted to try ci 4 lis but thought it was way t...|    1|      spam|wanted to try ci 4 lis but thought it was way t...|viagra at $ 1 . 12 per dose\\nready to boost you...|2004-09-11|\n",
      "|  3|      2209|enron / hpl actuals for december 11 , 2000 teco...|    0|       ham|        enron / hpl actuals for december 11 , 2000|teco tap 30 . 000 / enron ; 120 . 000 / hpl gas...|2000-12-12|\n",
      "|  4|     15880|looking for cheap high - quality software ? rot...|    1|      spam|looking for cheap high - quality software ? rot...|water past also , burn , course . gave country ...|2005-02-13|\n",
      "+---+----------+--------------------------------------------------+-----+----------+--------------------------------------------------+--------------------------------------------------+----------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 14:08:02 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , message_id, text, label, label_text, subject, message, date\n",
      " Schema: _c0, message_id, text, label, label_text, subject, message, date\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/epsilon/Desktop/folder_0/projects/sprint_3/Anti-Spam-Intelligent/DataSet_Emails.csv\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"DataSet_Emails.csv\", \n",
    "                    header=True, \n",
    "                    inferSchema=True, \n",
    "                    multiLine=True,  \n",
    "                    escape='\"'      \n",
    "                    )\n",
    "print(\"Dataset loaded successfully!\")\n",
    "df.show(5, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb134017",
   "metadata": {},
   "source": [
    "## 3. Examine Dataset Structure\n",
    "Display schema, dataset size, and sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d79a112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATASET STRUCTURE\n",
      "============================================================\n",
      "\n",
      "Column Types:\n",
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- message_id: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- label_text: string (nullable = true)\n",
      " |-- subject: string (nullable = true)\n",
      " |-- message: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 14:08:04 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , message_id, text, label, label_text, subject, message, date\n",
      " Schema: _c0, message_id, text, label, label_text, subject, message, date\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/epsilon/Desktop/folder_0/projects/sprint_3/Anti-Spam-Intelligent/DataSet_Emails.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total rows: 31716\n",
      "Total columns: 8\n",
      "\n",
      "Sample emails:\n",
      "+---+----------+----------------------------------------------------------------------------------------------------+-----+----------+---------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------+\n",
      "|_c0|message_id|                                                                                                text|label|label_text|                                                              subject|                                                                                             message|      date|\n",
      "+---+----------+----------------------------------------------------------------------------------------------------+-----+----------+---------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------+\n",
      "|  0|     33214|any software just for 15 $ - 99 $ understanding oem software\\nlead me not into temptation ; i can...|    1|      spam|                                    any software just for 15 $ - 99 $|understanding oem software\\nlead me not into temptation ; i can find the way myself .\\n# 3533 . t...|2005-06-18|\n",
      "|  1|     11929|perspective on ferc regulatory action client conf call today , jun e 19 th , 2 : 00 pm edt\\npersp...|    0|       ham| perspective on ferc regulatory action client conf call today , jun e|19 th , 2 : 00 pm edt\\nperspective on ferc regulatory action client conference call\\ntoday , tues...|2001-06-19|\n",
      "|  2|     19784|wanted to try ci 4 lis but thought it was way too expensive for you ? viagra at $ 1 . 12 per dose...|    1|      spam|wanted to try ci 4 lis but thought it was way too expensive for you ?|viagra at $ 1 . 12 per dose\\nready to boost your sex life ? positive ?\\ntime to do it right now ....|2004-09-11|\n",
      "+---+----------+----------------------------------------------------------------------------------------------------+-----+----------+---------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------+\n",
      "only showing top 3 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 14:08:04 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , message_id, text, label, label_text, subject, message, date\n",
      " Schema: _c0, message_id, text, label, label_text, subject, message, date\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/epsilon/Desktop/folder_0/projects/sprint_3/Anti-Spam-Intelligent/DataSet_Emails.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DATASET STRUCTURE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nColumn Types:\")\n",
    "df.printSchema()\n",
    "\n",
    "print(f\"\\nTotal rows: {df.count()}\")\n",
    "print(f\"Total columns: {len(df.columns)}\")\n",
    "\n",
    "print(\"\\nSample emails:\")\n",
    "df.show(3, truncate=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dc58fb",
   "metadata": {},
   "source": [
    "## 4. Detect and Handle Missing Values\n",
    "Analyze missing values in each column and remove rows with null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db33c3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MISSING VALUES ANALYSIS\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 14:08:08 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , message_id, text, label, label_text, subject, message, date\n",
      " Schema: _c0, message_id, text, label, label_text, subject, message, date\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/epsilon/Desktop/folder_0/projects/sprint_3/Anti-Spam-Intelligent/DataSet_Emails.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_c0: 0 missing values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 14:08:08 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , message_id, text, label, label_text, subject, message, date\n",
      " Schema: _c0, message_id, text, label, label_text, subject, message, date\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/epsilon/Desktop/folder_0/projects/sprint_3/Anti-Spam-Intelligent/DataSet_Emails.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message_id: 0 missing values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 14:08:08 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , message_id, text, label, label_text, subject, message, date\n",
      " Schema: _c0, message_id, text, label, label_text, subject, message, date\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/epsilon/Desktop/folder_0/projects/sprint_3/Anti-Spam-Intelligent/DataSet_Emails.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: 51 missing values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 14:08:09 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , message_id, text, label, label_text, subject, message, date\n",
      " Schema: _c0, message_id, text, label, label_text, subject, message, date\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/epsilon/Desktop/folder_0/projects/sprint_3/Anti-Spam-Intelligent/DataSet_Emails.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 0 missing values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 14:08:09 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , message_id, text, label, label_text, subject, message, date\n",
      " Schema: _c0, message_id, text, label, label_text, subject, message, date\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/epsilon/Desktop/folder_0/projects/sprint_3/Anti-Spam-Intelligent/DataSet_Emails.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_text: 0 missing values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 14:08:09 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , message_id, text, label, label_text, subject, message, date\n",
      " Schema: _c0, message_id, text, label, label_text, subject, message, date\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/epsilon/Desktop/folder_0/projects/sprint_3/Anti-Spam-Intelligent/DataSet_Emails.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject: 274 missing values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 14:08:10 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , message_id, text, label, label_text, subject, message, date\n",
      " Schema: _c0, message_id, text, label, label_text, subject, message, date\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/epsilon/Desktop/folder_0/projects/sprint_3/Anti-Spam-Intelligent/DataSet_Emails.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message: 345 missing values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 14:08:10 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , message_id, text, label, label_text, subject, message, date\n",
      " Schema: _c0, message_id, text, label, label_text, subject, message, date\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/epsilon/Desktop/folder_0/projects/sprint_3/Anti-Spam-Intelligent/DataSet_Emails.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date: 0 missing values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 14:08:10 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , message_id, text, label, label_text, subject, message, date\n",
      " Schema: _c0, message_id, text, label, label_text, subject, message, date\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/epsilon/Desktop/folder_0/projects/sprint_3/Anti-Spam-Intelligent/DataSet_Emails.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rows after removing missing values: 31148\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col , udf\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MISSING VALUES ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for column in df.columns:\n",
    "    null_count = df.filter(col(column).isNull()).count()\n",
    "    print(f\"{column}: {null_count} missing values\")\n",
    "\n",
    "df_clean = df.dropna()\n",
    "print(f\"\\nRows after removing missing values: {df_clean.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d365ae",
   "metadata": {},
   "source": [
    "## 5. Detect and Handle Duplicates\n",
    "Identify duplicate rows and remove them from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4438bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DUPLICATES ANALYSIS\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 12:04:15 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , message_id, text, label, label_text, subject, message, date\n",
      " Schema: _c0, message_id, text, label, label_text, subject, message, date\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/epsilon/Desktop/folder_0/projects/sprint_3/Anti-Spam-Intelligent/DataSet_Emails.csv\n",
      "25/11/26 12:04:15 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , message_id, text, label, label_text, subject, message, date\n",
      " Schema: _c0, message_id, text, label, label_text, subject, message, date\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/epsilon/Desktop/folder_0/projects/sprint_3/Anti-Spam-Intelligent/DataSet_Emails.csv\n",
      "25/11/26 12:04:16 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , message_id, text, label, label_text, subject, message, date\n",
      " Schema: _c0, message_id, text, label, label_text, subject, message, date\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/epsilon/Desktop/folder_0/projects/sprint_3/Anti-Spam-Intelligent/DataSet_Emails.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 31148\n",
      "Distinct rows: 31148\n",
      "Duplicates: 0\n",
      "\n",
      "Rows after removing duplicates: 31148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['_c0',\n",
       " 'message_id',\n",
       " 'text',\n",
       " 'label',\n",
       " 'label_text',\n",
       " 'subject',\n",
       " 'message',\n",
       " 'date']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DUPLICATES ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "total_rows = df_clean.count()\n",
    "distinct_rows = df_clean.distinct().count()\n",
    "duplicates = total_rows - distinct_rows\n",
    "\n",
    "print(f\"Total rows: {total_rows}\")\n",
    "print(f\"Distinct rows: {distinct_rows}\")\n",
    "print(f\"Duplicates: {duplicates}\")\n",
    "\n",
    "df_clean = df_clean.dropDuplicates()\n",
    "print(f\"\\nRows after removing duplicates: {df_clean.count()}\")\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d48dc3",
   "metadata": {},
   "source": [
    "## 6. Analyze Class Distribution\n",
    "Check the balance between spam and ham emails to identify potential class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c46116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CLASS DISTRIBUTION ANALYSIS\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 12:04:17 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , message_id, text, label, label_text, subject, message, date\n",
      " Schema: _c0, message_id, text, label, label_text, subject, message, date\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/epsilon/Desktop/folder_0/projects/sprint_3/Anti-Spam-Intelligent/DataSet_Emails.csv\n",
      "25/11/26 12:04:17 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , message_id, text, label, label_text, subject, message, date\n",
      " Schema: _c0, message_id, text, label, label_text, subject, message, date\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/epsilon/Desktop/folder_0/projects/sprint_3/Anti-Spam-Intelligent/DataSet_Emails.csv\n",
      "25/11/26 12:04:18 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , message_id, text, label, label_text, subject, message, date\n",
      " Schema: _c0, message_id, text, label, label_text, subject, message, date\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/epsilon/Desktop/folder_0/projects/sprint_3/Anti-Spam-Intelligent/DataSet_Emails.csv\n",
      "[Stage 210:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spam emails: 15645 (50.23%)\n",
      "Ham emails: 15503 (49.77%)\n",
      "Total emails: 31148\n",
      "\n",
      "Class imbalance ratio: 1:1.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CLASS DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "spam_count = df_clean.filter(col('label_text') == \"spam\").count()\n",
    "ham_count = df_clean.filter(col('label_text') == \"ham\").count()\n",
    "total = df_clean.count()\n",
    "\n",
    "print(f\"\\nSpam emails: {spam_count} ({spam_count/total*100:.2f}%)\")\n",
    "print(f\"Ham emails: {ham_count} ({ham_count/total*100:.2f}%)\")\n",
    "print(f\"Total emails: {total}\")\n",
    "\n",
    "if spam_count > ham_count:\n",
    "    ratio = spam_count / ham_count\n",
    "else:\n",
    "    ratio = ham_count / spam_count\n",
    "    \n",
    "print(f\"\\nClass imbalance ratio: 1:{ratio:.2f}\")\n",
    "if ratio > 2:\n",
    "    print(\"âš  Dataset is imbalanced - consider using techniques like SMOTE or class weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aab90ce",
   "metadata": {},
   "source": [
    "## 7. Collect Text Data by Class\n",
    "Gather all spam and ham text for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fcd7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 12:04:19 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , message_id, text, label, label_text, subject, message, date\n",
      " Schema: _c0, message_id, text, label, label_text, subject, message, date\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/epsilon/Desktop/folder_0/projects/sprint_3/Anti-Spam-Intelligent/DataSet_Emails.csv\n",
      "25/11/26 12:04:20 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , message_id, text, label, label_text, subject, message, date\n",
      " Schema: _c0, message_id, text, label, label_text, subject, message, date\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/epsilon/Desktop/folder_0/projects/sprint_3/Anti-Spam-Intelligent/DataSet_Emails.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam text collected: 88474 characters\n",
      "Ham text collected: 87469 characters\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower, regexp_replace, concat_ws\n",
    "\n",
    "text_column = [col for col in df_clean.columns if col != 'label_text'][0]\n",
    "\n",
    "spam_text = df_clean.filter(col('label_text') == 'spam').select(text_column).rdd.map(lambda x: x[0]).collect()\n",
    "spam_text_combined = ' '.join([str(text) for text in spam_text if text])\n",
    "\n",
    "ham_text = df_clean.filter(col('label_text') == 'ham').select(text_column).rdd.map(lambda x: x[0]).collect()\n",
    "ham_text_combined = ' '.join([str(text) for text in ham_text if text])\n",
    "\n",
    "print(f\"Spam text collected: {len(spam_text_combined)} characters\")\n",
    "print(f\"Ham text collected: {len(ham_text_combined)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f152d0",
   "metadata": {},
   "source": [
    "## 8. Text Preprocessing Setup (Initial Attempt)\n",
    "First attempt at setting up text preprocessing with NLTK and lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3c883c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/epsilon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/epsilon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/epsilon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/epsilon/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "normalize_word = WordNetLemmatizer()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = regexp_replace(text, r'http\\S+|www\\S+|https\\S+', 'url')\n",
    "    text = regexp_replace(text, r'\\d+', 'number')\n",
    "    text = regexp_replace(text, r'[^\\w\\s]', '')\n",
    "    text = text.lower()\n",
    "    \n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    normalized_word = [normalize_word.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    \n",
    "    return normalize_word\n",
    "\n",
    "vectorize = TfidfVectorizer(tokenizer=preprocess_text, token_pattern=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad6ad38",
   "metadata": {},
   "source": [
    "## 9. Create UDF for Text Preprocessing\n",
    "Apply the preprocessing function as a user-defined function in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec1264a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "clean_udf = udf(preprocess_text, ArrayType(StringType()))\n",
    "new_df = df_clean.withColumn(\"tokens\", clean_udf(col(\"text\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c50cf1",
   "metadata": {},
   "source": [
    "## 10. Text Preprocessing (Improved Implementation)\n",
    "Enhanced text preprocessing with proper handling of null values and combined text fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15c2eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/epsilon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/epsilon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/epsilon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/epsilon/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/epsilon/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "25/11/26 14:16:01 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , message_id, text, label, label_text, subject, message, date\n",
      " Schema: _c0, message_id, text, label, label_text, subject, message, date\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/epsilon/Desktop/folder_0/projects/sprint_3/Anti-Spam-Intelligent/DataSet_Emails.csv\n",
      "[Stage 36:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|label|label_text|tokens                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "+-----+----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1    |spam      |[software, number, number, understanding, oem, software, lead, temptation, find, way, number, law, disregard, trifle, software, number, number, understanding, oem, software, lead, temptation, find, way, number, law, disregard, trifle]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "|0    |ham       |[perspective, ferc, regulatory, action, client, conf, call, today, jun, e, number, th, number, number, pm, edt, perspective, ferc, regulatory, action, client, conference, call, today, tuesday, june, number, th, number, number, pm, edt, host, ray, nile, power, natural, gas, analyst, speaker, steve, bergstrom, president, coo, dynegy, steve, bergstrom, president, chief, operating, officer, dynegy, join, u, number, number, p, today, conference, call, discussion, recent, ferc, action, imposing, price, control, west, discussion, followed, q, question, explored, include, implication, ferc, action, dyn, industry, whole, earnings, impact, risk, regulation, whatever, else, mind, attach, two, recent, note, ferc, action, reference, call, replay, reservation, number, number, number, u, number, number, number, u, number, number, number, number, int, l, number, number, number, int, l, replay, number, number, number, number, pm, raymond, c, nile, power, natural, gas, research, salomon, smith, barney, number, number, number, ray, nile, ssmb, com, perspective, ferc, regulatory, action, client, conf, call, today, jun, e, number, th, number, number, pm, edt, perspective, ferc, regulatory, action, client, conference, call, today, tuesday, june, number, th, number, number, pm, edt, host, ray, nile, power, natural, gas, analyst, speaker, steve, bergstrom, president, coo, dynegy, steve, bergstrom, president, chief, operating, officer, dynegy, join, u, number, number, p, today, conference, call, discussion, recent, ferc, action, imposing, price, control, west, discussion, followed, q, question, explored, include, implication, ferc, action, dyn, industry, whole, earnings, impact, risk, regulation, whatever, else, mind, attach, two, recent, note, ferc, action, reference, call, replay, reservation, number, number, number, u, number, number, number, u, number, number, number, number, int, l, number, number, number, int, l, replay, number, number, number, number, pm, raymond, c, nile, power, natural, gas, research, salomon, smith, barney, number, number, number, ray, nile, ssmb, com]|\n",
      "|1    |spam      |[wanted, try, ci, number, li, thought, way, expensive, viagra, number, number, per, dose, ready, boost, sex, life, positive, time, right, order, viagra, incredibly, low, price, number, number, per, dose, unbelivable, remove, wanted, try, ci, number, li, thought, way, expensive, viagra, number, number, per, dose, ready, boost, sex, life, positive, time, right, order, viagra, incredibly, low, price, number, number, per, dose, unbelivable, remove]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|0    |ham       |[enron, hpl, actuals, december, number, number, teco, tap, number, number, enron, number, number, hpl, gas, daily, l, hpl, lsk, ic, number, number, enron, enron, hpl, actuals, december, number, number, teco, tap, number, number, enron, number, number, hpl, gas, daily, l, hpl, lsk, ic, number, number, enron]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|1    |spam      |[looking, cheap, high, quality, software, rotated, napoleonizes, water, past, also, burn, course, gave, country, mass, lot, act, north, good, learn, form, brother, vary, stick, century, put, song, test, describe, plain, wood, star, began, dress, ever, group, oh, world, stay, looking, cheap, high, quality, software, rotated, napoleonizes, water, past, also, burn, course, gave, country, mass, lot, act, north, good, learn, form, brother, vary, stick, century, put, song, test, describe, plain, wood, star, began, dress, ever, group, oh, world, stay]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "+-----+----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from pyspark.sql.functions import udf, col, concat_ws, coalesce, lit\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if text is None or text == \"\":\n",
    "        return []\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', 'url', text)\n",
    "    text = re.sub(r'\\d+', 'number', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    normalized_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and token.strip()]\n",
    "    return normalized_tokens\n",
    "\n",
    "clean_udf = udf(preprocess_text, ArrayType(StringType()))\n",
    "\n",
    "cleaned_df = df_clean.withColumn(\n",
    "    \"all_text\",\n",
    "    concat_ws(\" \", coalesce(col(\"text\"), lit(\"\")), coalesce(col(\"subject\"), lit(\"\")), coalesce(col(\"message\"), lit(\"\")))\n",
    ").withColumn(\"tokens\", clean_udf(col(\"all_text\"))).select(\"label\", \"label_text\", \"tokens\")\n",
    "\n",
    "cleaned_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ecb9c8",
   "metadata": {},
   "source": [
    "## 11. Generate Word Clouds\n",
    "Visualize the most frequent words in spam and ham emails using word clouds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2244a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    from wordcloud import WordCloud\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"wordcloud\"])\n",
    "    from wordcloud import WordCloud\n",
    "\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"WORDCLOUD GENERATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "spam_tokens = (cleaned_df.filter(col(\"label_text\") == \"spam\")\n",
    "               .select(\"tokens\")\n",
    "               .rdd.flatMap(lambda r: r[0] if r[0] else [])\n",
    "               .collect())\n",
    "\n",
    "ham_tokens = (cleaned_df.filter(col(\"label_text\") == \"ham\")\n",
    "              .select(\"tokens\")\n",
    "              .rdd.flatMap(lambda r: r[0] if r[0] else [])\n",
    "              .collect())\n",
    "\n",
    "print(f\"\\nSpam tokens collected: {len(spam_tokens)}\")\n",
    "print(f\"Ham tokens collected: {len(ham_tokens)}\")\n",
    "\n",
    "spam_freq = Counter(spam_tokens)\n",
    "ham_freq = Counter(ham_tokens)\n",
    "\n",
    "print(f\"\\nTop 10 spam words: {spam_freq.most_common(10)}\")\n",
    "print(f\"Top 10 ham words: {ham_freq.most_common(10)}\")\n",
    "\n",
    "wc_params = dict(\n",
    "    width=800, \n",
    "    height=400, \n",
    "    background_color=\"white\", \n",
    "    collocations=False,\n",
    "    max_words=100\n",
    ")\n",
    "\n",
    "spam_wc = WordCloud(**wc_params).generate_from_frequencies(spam_freq)\n",
    "ham_wc = WordCloud(**wc_params).generate_from_frequencies(ham_freq)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(spam_wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Spam Emails - Most Frequent Words\", fontsize=16, fontweight='bold')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(ham_wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Ham Emails - Most Frequent Words\", fontsize=16, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "spam_wc.to_file(\"spam_wordcloud.png\")\n",
    "ham_wc.to_file(\"ham_wordcloud.png\")\n",
    "print(\"\\nâœ“ WordClouds saved as 'spam_wordcloud.png' and 'ham_wordcloud.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6b3e03",
   "metadata": {},
   "source": [
    "## 12. Check DataFrame Columns\n",
    "Display available columns in the processed dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565f72a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0',\n",
       " 'message_id',\n",
       " 'text',\n",
       " 'label',\n",
       " 'label_text',\n",
       " 'subject',\n",
       " 'message',\n",
       " 'date',\n",
       " 'combined_text',\n",
       " 'text_clean',\n",
       " 'tokens']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3691532",
   "metadata": {},
   "source": [
    "## 13. Text Vectorization\n",
    "Convert processed text into numerical features using TF-IDF and Count Vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2b1eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEXT VECTORIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "texts_for_vectorization = cleaned_df.select(\n",
    "    concat_ws(\" \", col(\"tokens\")).alias(\"processed_text\"),\n",
    "    \"label\"\n",
    ").toPandas()\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(texts_for_vectorization['processed_text'])\n",
    "print(f\"\\nTF-IDF Matrix shape: {X_tfidf.shape}\")\n",
    "print(f\"Number of features: {len(tfidf_vectorizer.get_feature_names_out())}\")\n",
    "\n",
    "count_vectorizer = CountVectorizer(max_features=5000)\n",
    "X_count = count_vectorizer.fit_transform(texts_for_vectorization['processed_text'])\n",
    "print(f\"\\nCount Matrix shape: {X_count.shape}\")\n",
    "print(f\"Number of features: {len(count_vectorizer.get_feature_names_out())}\")\n",
    "\n",
    "y = texts_for_vectorization['label'].values\n",
    "\n",
    "print(\"\\nâœ“ Vectorization completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
