{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78f18ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/26 14:07:57 WARN Utils: Your hostname, epsilon, resolves to a loopback address: 127.0.1.1; using 192.168.9.119 instead (on interface wlp2s0f0)\n",
      "25/11/26 14:07:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/26 14:07:57 WARN Utils: Your hostname, epsilon, resolves to a loopback address: 127.0.1.1; using 192.168.9.119 instead (on interface wlp2s0f0)\n",
      "25/11/26 14:07:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/26 14:07:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/26 14:07:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ExampleApp\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "994a99af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "+---+----------+--------------------------------------------------+-----+----------+--------------------------------------------------+--------------------------------------------------+----------+\n",
      "|_c0|message_id|                                              text|label|label_text|                                           subject|                                           message|      date|\n",
      "+---+----------+--------------------------------------------------+-----+----------+--------------------------------------------------+--------------------------------------------------+----------+\n",
      "|  0|     33214|any software just for 15 $ - 99 $ understanding...|    1|      spam|                 any software just for 15 $ - 99 $|understanding oem software\\nlead me not into te...|2005-06-18|\n",
      "|  1|     11929|perspective on ferc regulatory action client co...|    0|       ham|perspective on ferc regulatory action client co...|19 th , 2 : 00 pm edt\\nperspective on ferc regu...|2001-06-19|\n",
      "|  2|     19784|wanted to try ci 4 lis but thought it was way t...|    1|      spam|wanted to try ci 4 lis but thought it was way t...|viagra at $ 1 . 12 per dose\\nready to boost you...|2004-09-11|\n",
      "|  3|      2209|enron / hpl actuals for december 11 , 2000 teco...|    0|       ham|        enron / hpl actuals for december 11 , 2000|teco tap 30 . 000 / enron ; 120 . 000 / hpl gas...|2000-12-12|\n",
      "|  4|     15880|looking for cheap high - quality software ? rot...|    1|      spam|looking for cheap high - quality software ? rot...|water past also , burn , course . gave country ...|2005-02-13|\n",
      "+---+----------+--------------------------------------------------+-----+----------+--------------------------------------------------+--------------------------------------------------+----------+\n",
      "only showing top 5 rows\n",
      "+---+----------+--------------------------------------------------+-----+----------+--------------------------------------------------+--------------------------------------------------+----------+\n",
      "|_c0|message_id|                                              text|label|label_text|                                           subject|                                           message|      date|\n",
      "+---+----------+--------------------------------------------------+-----+----------+--------------------------------------------------+--------------------------------------------------+----------+\n",
      "|  0|     33214|any software just for 15 $ - 99 $ understanding...|    1|      spam|                 any software just for 15 $ - 99 $|understanding oem software\\nlead me not into te...|2005-06-18|\n",
      "|  1|     11929|perspective on ferc regulatory action client co...|    0|       ham|perspective on ferc regulatory action client co...|19 th , 2 : 00 pm edt\\nperspective on ferc regu...|2001-06-19|\n",
      "|  2|     19784|wanted to try ci 4 lis but thought it was way t...|    1|      spam|wanted to try ci 4 lis but thought it was way t...|viagra at $ 1 . 12 per dose\\nready to boost you...|2004-09-11|\n",
      "|  3|      2209|enron / hpl actuals for december 11 , 2000 teco...|    0|       ham|        enron / hpl actuals for december 11 , 2000|teco tap 30 . 000 / enron ; 120 . 000 / hpl gas...|2000-12-12|\n",
      "|  4|     15880|looking for cheap high - quality software ? rot...|    1|      spam|looking for cheap high - quality software ? rot...|water past also , burn , course . gave country ...|2005-02-13|\n",
      "+---+----------+--------------------------------------------------+-----+----------+--------------------------------------------------+--------------------------------------------------+----------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 14:08:02 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , message_id, text, label, label_text, subject, message, date\n",
      " Schema: _c0, message_id, text, label, label_text, subject, message, date\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/epsilon/Desktop/folder_0/projects/sprint_3/Anti-Spam-Intelligent/DataSet_Emails.csv\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = spark.read.csv(\"DataSet_Emails.csv\", \n",
    "                    header=True, \n",
    "                    inferSchema=True, \n",
    "                    multiLine=True,  \n",
    "                    escape='\"'      \n",
    "                    )\n",
    "print(\"Dataset loaded successfully!\")\n",
    "df.show(5, truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d79a112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATASET STRUCTURE\n",
      "============================================================\n",
      "\n",
      "Column Types:\n",
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- message_id: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- label_text: string (nullable = true)\n",
      " |-- subject: string (nullable = true)\n",
      " |-- message: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 14:08:04 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , message_id, text, label, label_text, subject, message, date\n",
      " Schema: _c0, message_id, text, label, label_text, subject, message, date\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/epsilon/Desktop/folder_0/projects/sprint_3/Anti-Spam-Intelligent/DataSet_Emails.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total rows: 31716\n",
      "Total columns: 8\n",
      "\n",
      "Sample emails:\n",
      "+---+----------+----------------------------------------------------------------------------------------------------+-----+----------+---------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------+\n",
      "|_c0|message_id|                                                                                                text|label|label_text|                                                              subject|                                                                                             message|      date|\n",
      "+---+----------+----------------------------------------------------------------------------------------------------+-----+----------+---------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------+\n",
      "|  0|     33214|any software just for 15 $ - 99 $ understanding oem software\\nlead me not into temptation ; i can...|    1|      spam|                                    any software just for 15 $ - 99 $|understanding oem software\\nlead me not into temptation ; i can find the way myself .\\n# 3533 . t...|2005-06-18|\n",
      "|  1|     11929|perspective on ferc regulatory action client conf call today , jun e 19 th , 2 : 00 pm edt\\npersp...|    0|       ham| perspective on ferc regulatory action client conf call today , jun e|19 th , 2 : 00 pm edt\\nperspective on ferc regulatory action client conference call\\ntoday , tues...|2001-06-19|\n",
      "|  2|     19784|wanted to try ci 4 lis but thought it was way too expensive for you ? viagra at $ 1 . 12 per dose...|    1|      spam|wanted to try ci 4 lis but thought it was way too expensive for you ?|viagra at $ 1 . 12 per dose\\nready to boost your sex life ? positive ?\\ntime to do it right now ....|2004-09-11|\n",
      "+---+----------+----------------------------------------------------------------------------------------------------+-----+----------+---------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------+\n",
      "only showing top 3 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 14:08:04 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , message_id, text, label, label_text, subject, message, date\n",
      " Schema: _c0, message_id, text, label, label_text, subject, message, date\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/epsilon/Desktop/folder_0/projects/sprint_3/Anti-Spam-Intelligent/DataSet_Emails.csv\n"
     ]
    }
   ],
   "source": [
    "# 1. EXAMINE DATASET STRUCTURE\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET STRUCTURE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Display schema\n",
    "print(\"\\nColumn Types:\")\n",
    "df.printSchema()\n",
    "\n",
    "# Dataset size\n",
    "print(f\"\\nTotal rows: {df.count()}\")\n",
    "print(f\"Total columns: {len(df.columns)}\")\n",
    "\n",
    "# Show sample data\n",
    "print(\"\\nSample emails:\")\n",
    "df.show(3, truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db33c3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MISSING VALUES ANALYSIS\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 14:08:08 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , message_id, text, label, label_text, subject, message, date\n",
      " Schema: _c0, message_id, text, label, label_text, subject, message, date\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/epsilon/Desktop/folder_0/projects/sprint_3/Anti-Spam-Intelligent/DataSet_Emails.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_c0: 0 missing values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 14:08:08 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , message_id, text, label, label_text, subject, message, date\n",
      " Schema: _c0, message_id, text, label, label_text, subject, message, date\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/epsilon/Desktop/folder_0/projects/sprint_3/Anti-Spam-Intelligent/DataSet_Emails.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message_id: 0 missing values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 14:08:08 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , message_id, text, label, label_text, subject, message, date\n",
      " Schema: _c0, message_id, text, label, label_text, subject, message, date\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/epsilon/Desktop/folder_0/projects/sprint_3/Anti-Spam-Intelligent/DataSet_Emails.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: 51 missing values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 14:08:09 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , message_id, text, label, label_text, subject, message, date\n",
      " Schema: _c0, message_id, text, label, label_text, subject, message, date\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/epsilon/Desktop/folder_0/projects/sprint_3/Anti-Spam-Intelligent/DataSet_Emails.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 0 missing values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 14:08:09 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , message_id, text, label, label_text, subject, message, date\n",
      " Schema: _c0, message_id, text, label, label_text, subject, message, date\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/epsilon/Desktop/folder_0/projects/sprint_3/Anti-Spam-Intelligent/DataSet_Emails.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_text: 0 missing values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 14:08:09 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , message_id, text, label, label_text, subject, message, date\n",
      " Schema: _c0, message_id, text, label, label_text, subject, message, date\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/epsilon/Desktop/folder_0/projects/sprint_3/Anti-Spam-Intelligent/DataSet_Emails.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject: 274 missing values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 14:08:10 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , message_id, text, label, label_text, subject, message, date\n",
      " Schema: _c0, message_id, text, label, label_text, subject, message, date\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/epsilon/Desktop/folder_0/projects/sprint_3/Anti-Spam-Intelligent/DataSet_Emails.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message: 345 missing values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 14:08:10 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , message_id, text, label, label_text, subject, message, date\n",
      " Schema: _c0, message_id, text, label, label_text, subject, message, date\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/epsilon/Desktop/folder_0/projects/sprint_3/Anti-Spam-Intelligent/DataSet_Emails.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date: 0 missing values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 14:08:10 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , message_id, text, label, label_text, subject, message, date\n",
      " Schema: _c0, message_id, text, label, label_text, subject, message, date\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/epsilon/Desktop/folder_0/projects/sprint_3/Anti-Spam-Intelligent/DataSet_Emails.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rows after removing missing values: 31148\n"
     ]
    }
   ],
   "source": [
    "# 2. DETECT AND HANDLE MISSING VALUES\n",
    "from pyspark.sql.functions import col , udf\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MISSING VALUES ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for column in df.columns:\n",
    "    null_count = df.filter(col(column).isNull()).count()\n",
    "    print(f\"{column}: {null_count} missing values\")\n",
    "\n",
    "df_clean = df.dropna()\n",
    "print(f\"\\nRows after removing missing values: {df_clean.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4438bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DUPLICATES ANALYSIS\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 12:04:15 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , message_id, text, label, label_text, subject, message, date\n",
      " Schema: _c0, message_id, text, label, label_text, subject, message, date\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/epsilon/Desktop/folder_0/projects/sprint_3/Anti-Spam-Intelligent/DataSet_Emails.csv\n",
      "25/11/26 12:04:15 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , message_id, text, label, label_text, subject, message, date\n",
      " Schema: _c0, message_id, text, label, label_text, subject, message, date\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/epsilon/Desktop/folder_0/projects/sprint_3/Anti-Spam-Intelligent/DataSet_Emails.csv\n",
      "25/11/26 12:04:16 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , message_id, text, label, label_text, subject, message, date\n",
      " Schema: _c0, message_id, text, label, label_text, subject, message, date\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/epsilon/Desktop/folder_0/projects/sprint_3/Anti-Spam-Intelligent/DataSet_Emails.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 31148\n",
      "Distinct rows: 31148\n",
      "Duplicates: 0\n",
      "\n",
      "Rows after removing duplicates: 31148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['_c0',\n",
       " 'message_id',\n",
       " 'text',\n",
       " 'label',\n",
       " 'label_text',\n",
       " 'subject',\n",
       " 'message',\n",
       " 'date']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. DETECT AND HANDLE DUPLICATES\n",
    "print(\"=\" * 60)\n",
    "print(\"DUPLICATES ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "total_rows = df_clean.count()\n",
    "distinct_rows = df_clean.distinct().count()\n",
    "duplicates = total_rows - distinct_rows\n",
    "\n",
    "print(f\"Total rows: {total_rows}\")\n",
    "print(f\"Distinct rows: {distinct_rows}\")\n",
    "print(f\"Duplicates: {duplicates}\")\n",
    "\n",
    "# Remove duplicates\n",
    "df_clean = df_clean.dropDuplicates()\n",
    "print(f\"\\nRows after removing duplicates: {df_clean.count()}\")\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "14c46116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CLASS DISTRIBUTION ANALYSIS\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 12:04:17 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , message_id, text, label, label_text, subject, message, date\n",
      " Schema: _c0, message_id, text, label, label_text, subject, message, date\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/epsilon/Desktop/folder_0/projects/sprint_3/Anti-Spam-Intelligent/DataSet_Emails.csv\n",
      "25/11/26 12:04:17 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , message_id, text, label, label_text, subject, message, date\n",
      " Schema: _c0, message_id, text, label, label_text, subject, message, date\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/epsilon/Desktop/folder_0/projects/sprint_3/Anti-Spam-Intelligent/DataSet_Emails.csv\n",
      "25/11/26 12:04:18 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , message_id, text, label, label_text, subject, message, date\n",
      " Schema: _c0, message_id, text, label, label_text, subject, message, date\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/epsilon/Desktop/folder_0/projects/sprint_3/Anti-Spam-Intelligent/DataSet_Emails.csv\n",
      "[Stage 210:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spam emails: 15645 (50.23%)\n",
      "Ham emails: 15503 (49.77%)\n",
      "Total emails: 31148\n",
      "\n",
      "Class imbalance ratio: 1:1.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 4. ANALYZE CLASS DISTRIBUTION\n",
    "print(\"=\" * 60)\n",
    "print(\"CLASS DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count spam vs ham\n",
    "spam_count = df_clean.filter(col('label_text') == \"spam\").count()\n",
    "ham_count = df_clean.filter(col('label_text') == \"ham\").count()\n",
    "total = df_clean.count()\n",
    "\n",
    "print(f\"\\nSpam emails: {spam_count} ({spam_count/total*100:.2f}%)\")\n",
    "print(f\"Ham emails: {ham_count} ({ham_count/total*100:.2f}%)\")\n",
    "print(f\"Total emails: {total}\")\n",
    "\n",
    "# Check balance\n",
    "if spam_count > ham_count:\n",
    "    ratio = spam_count / ham_count\n",
    "else:\n",
    "    ratio = ham_count / spam_count\n",
    "    \n",
    "print(f\"\\nClass imbalance ratio: 1:{ratio:.2f}\")\n",
    "if ratio > 2:\n",
    "    print(\"âš  Dataset is imbalanced - consider using techniques like SMOTE or class weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "53fcd7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 12:04:19 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , message_id, text, label, label_text, subject, message, date\n",
      " Schema: _c0, message_id, text, label, label_text, subject, message, date\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/epsilon/Desktop/folder_0/projects/sprint_3/Anti-Spam-Intelligent/DataSet_Emails.csv\n",
      "25/11/26 12:04:20 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , message_id, text, label, label_text, subject, message, date\n",
      " Schema: _c0, message_id, text, label, label_text, subject, message, date\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/epsilon/Desktop/folder_0/projects/sprint_3/Anti-Spam-Intelligent/DataSet_Emails.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam text collected: 88474 characters\n",
      "Ham text collected: 87469 characters\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower, regexp_replace, concat_ws\n",
    "\n",
    "text_column = [col for col in df_clean.columns if col != 'label_text'][0]\n",
    "\n",
    "spam_text = df_clean.filter(col('label_text') == 'spam').select(text_column).rdd.map(lambda x: x[0]).collect()\n",
    "spam_text_combined = ' '.join([str(text) for text in spam_text if text])\n",
    "\n",
    "ham_text = df_clean.filter(col('label_text') == 'ham').select(text_column).rdd.map(lambda x: x[0]).collect()\n",
    "ham_text_combined = ' '.join([str(text) for text in ham_text if text])\n",
    "\n",
    "print(f\"Spam text collected: {len(spam_text_combined)} characters\")\n",
    "print(f\"Ham text collected: {len(ham_text_combined)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c3c883c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/epsilon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/epsilon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/epsilon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/epsilon/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "normalize_word = WordNetLemmatizer()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = regexp_replace(text, r'http\\S+|www\\S+|https\\S+', 'url')\n",
    "    text = regexp_replace(text, r'\\d+', 'number')\n",
    "    text = regexp_replace(text, r'[^\\w\\s]', '')\n",
    "    text = text.lower()\n",
    "    \n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    normalized_word = [normalize_word.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    \n",
    "    return normalize_word\n",
    "\n",
    "vectorize = TfidfVectorizer(tokenizer=preprocess_text, token_pattern=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2ec1264a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "clean_udf = udf(preprocess_text, ArrayType(StringType()))\n",
    "new_df = df_clean.withColumn(\"tokens\", clean_udf(col(\"text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c15c2eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/epsilon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/epsilon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/epsilon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/epsilon/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/epsilon/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "25/11/26 14:16:01 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , message_id, text, label, label_text, subject, message, date\n",
      " Schema: _c0, message_id, text, label, label_text, subject, message, date\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/epsilon/Desktop/folder_0/projects/sprint_3/Anti-Spam-Intelligent/DataSet_Emails.csv\n",
      "[Stage 36:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|label|label_text|tokens                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "+-----+----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1    |spam      |[software, number, number, understanding, oem, software, lead, temptation, find, way, number, law, disregard, trifle, software, number, number, understanding, oem, software, lead, temptation, find, way, number, law, disregard, trifle]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "|0    |ham       |[perspective, ferc, regulatory, action, client, conf, call, today, jun, e, number, th, number, number, pm, edt, perspective, ferc, regulatory, action, client, conference, call, today, tuesday, june, number, th, number, number, pm, edt, host, ray, nile, power, natural, gas, analyst, speaker, steve, bergstrom, president, coo, dynegy, steve, bergstrom, president, chief, operating, officer, dynegy, join, u, number, number, p, today, conference, call, discussion, recent, ferc, action, imposing, price, control, west, discussion, followed, q, question, explored, include, implication, ferc, action, dyn, industry, whole, earnings, impact, risk, regulation, whatever, else, mind, attach, two, recent, note, ferc, action, reference, call, replay, reservation, number, number, number, u, number, number, number, u, number, number, number, number, int, l, number, number, number, int, l, replay, number, number, number, number, pm, raymond, c, nile, power, natural, gas, research, salomon, smith, barney, number, number, number, ray, nile, ssmb, com, perspective, ferc, regulatory, action, client, conf, call, today, jun, e, number, th, number, number, pm, edt, perspective, ferc, regulatory, action, client, conference, call, today, tuesday, june, number, th, number, number, pm, edt, host, ray, nile, power, natural, gas, analyst, speaker, steve, bergstrom, president, coo, dynegy, steve, bergstrom, president, chief, operating, officer, dynegy, join, u, number, number, p, today, conference, call, discussion, recent, ferc, action, imposing, price, control, west, discussion, followed, q, question, explored, include, implication, ferc, action, dyn, industry, whole, earnings, impact, risk, regulation, whatever, else, mind, attach, two, recent, note, ferc, action, reference, call, replay, reservation, number, number, number, u, number, number, number, u, number, number, number, number, int, l, number, number, number, int, l, replay, number, number, number, number, pm, raymond, c, nile, power, natural, gas, research, salomon, smith, barney, number, number, number, ray, nile, ssmb, com]|\n",
      "|1    |spam      |[wanted, try, ci, number, li, thought, way, expensive, viagra, number, number, per, dose, ready, boost, sex, life, positive, time, right, order, viagra, incredibly, low, price, number, number, per, dose, unbelivable, remove, wanted, try, ci, number, li, thought, way, expensive, viagra, number, number, per, dose, ready, boost, sex, life, positive, time, right, order, viagra, incredibly, low, price, number, number, per, dose, unbelivable, remove]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|0    |ham       |[enron, hpl, actuals, december, number, number, teco, tap, number, number, enron, number, number, hpl, gas, daily, l, hpl, lsk, ic, number, number, enron, enron, hpl, actuals, december, number, number, teco, tap, number, number, enron, number, number, hpl, gas, daily, l, hpl, lsk, ic, number, number, enron]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|1    |spam      |[looking, cheap, high, quality, software, rotated, napoleonizes, water, past, also, burn, course, gave, country, mass, lot, act, north, good, learn, form, brother, vary, stick, century, put, song, test, describe, plain, wood, star, began, dress, ever, group, oh, world, stay, looking, cheap, high, quality, software, rotated, napoleonizes, water, past, also, burn, course, gave, country, mass, lot, act, north, good, learn, form, brother, vary, stick, century, put, song, test, describe, plain, wood, star, began, dress, ever, group, oh, world, stay]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "+-----+----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from pyspark.sql.functions import udf, col, concat_ws, coalesce, lit\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if text is None or text == \"\":\n",
    "        return []\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', 'url', text)\n",
    "    text = re.sub(r'\\d+', 'number', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    normalized_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and token.strip()]\n",
    "    return normalized_tokens\n",
    "\n",
    "clean_udf = udf(preprocess_text, ArrayType(StringType()))\n",
    "\n",
    "cleaned_df = df_clean.withColumn(\n",
    "    \"all_text\",\n",
    "    concat_ws(\" \", coalesce(col(\"text\"), lit(\"\")), coalesce(col(\"subject\"), lit(\"\")), coalesce(col(\"message\"), lit(\"\")))\n",
    ").withColumn(\"tokens\", clean_udf(col(\"all_text\"))).select(\"label\", \"label_text\", \"tokens\")\n",
    "\n",
    "cleaned_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "565f72a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0',\n",
       " 'message_id',\n",
       " 'text',\n",
       " 'label',\n",
       " 'label_text',\n",
       " 'subject',\n",
       " 'message',\n",
       " 'date',\n",
       " 'combined_text',\n",
       " 'text_clean',\n",
       " 'tokens']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2b1eb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
